{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade transformers accelerate safetensors einops\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import kagglehub\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 | Load our data set\n",
    "we are using the winemag wine review data set that contains 130k wine reviews from several reviewers on wines from all over the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"zynicide/wine-reviews\")\n",
    "df = pd.read_csv(path + \"/winemag-data-130k-v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 | Generate Synthetic User Queries\n",
    "in order to train a Bert based model to fine semantic similarity between user queries and the desired wine we need to create good and bad samples of what search queries is paired with which wine\n",
    "because we don't have those already in our data set we will use prompt engineering along with so post processing to create the sample user queries we expect to see\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model we want to use for generating samples\n",
    "#we chose Qwen because its a free model we have had experience with in the past\n",
    "\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.sliding_window = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim the query to a max of 3 parts even though we tell the model to do this\n",
    "#it sometimes likes to not do that so this is a safety net\n",
    "def trim_query(text, max_parts=3):\n",
    "    # Lowercase the text for normalization\n",
    "    text = text.strip()\n",
    "\n",
    "    # Split by common descriptor joiners\n",
    "    parts = re.split(r'\\s*(?:,| and | with |\\. )\\s*', text)\n",
    "\n",
    "    # Keep only the first few parts\n",
    "    trimmed_parts = parts[:max_parts]\n",
    "\n",
    "    # Re-join them with commas or 'and' for flow\n",
    "    if len(trimmed_parts) > 1:\n",
    "        result = ', '.join(trimmed_parts[:-1]) + ' and ' + trimmed_parts[-1]\n",
    "    else:\n",
    "        result = trimmed_parts[0]\n",
    "\n",
    "    return result.strip().capitalize()\n",
    "\n",
    "def generate_sample_query(wine_desc):\n",
    "    \n",
    "    #inorder to prevent the model from generating responses that are too similar to each other we create a list of prompt variants to choose from\n",
    "    #this is a list of 10 different prompts that are all similar but slightly different\n",
    "    prompt_variants = [\n",
    "    \"\"\"You are a helpful assistant that creates short, human-like wine search queries from wine descriptions. The queries should be realistic, not too detailed, and focus on 1–3 key aspects of the wine.\"\"\",\n",
    "\n",
    "    \"\"\"You are an assistant that turns wine descriptions into simple, casual search phrases that a real person might use. Do not list everything—just highlight a few main traits.\"\"\",\n",
    "\n",
    "    \"\"\"You generate concise, informal wine search queries based on tasting notes. Keep it brief and include only the most noticeable characteristics.\"\"\",\n",
    "\n",
    "    \"\"\"You're helping wine lovers describe wines in simple search terms. Summarize the wine's essence in 1–2 quick flavor notes or attributes.\"\"\",\n",
    "\n",
    "    \"\"\"You generate short, realistic search queries from wine descriptions. Keep the language natural and intuitive. Mention just a couple key traits.\"\"\",\n",
    "\n",
    "    \"\"\"Your job is to turn wine descriptions into casual search-style queries using only the most important descriptors. Avoid over-detailing or repetition.\"\"\",\n",
    "\n",
    "    \"\"\"You create short user-style search queries based on wine descriptions. Stick to 1–3 high-impact descriptors. Be realistic and natural, not formal.\"\"\",\n",
    "\n",
    "    \"\"\"You are generating imperfect but useful search queries from wine descriptions. The goal is to sound like a real wine shopper, not list every flavor.\"\"\",\n",
    "\n",
    "    \"\"\"Turn the wine descriptions into realistic, search-friendly phrases a person might type. Include only standout flavors or textures.\"\"\",\n",
    "\n",
    "    \"\"\"You're writing very short, human-friendly search terms based on full wine descriptions. Focus on the most prominent flavors or textures.\"\"\"\n",
    "    ]   \n",
    "\n",
    "    # Select a random prompt variant\n",
    "    prompt = prompt_variants[random.randint(0, len(prompt_variants) - 1)]\n",
    "    \n",
    "    #this is the prompt we use to generate the query \n",
    "    #we use the randomly selected prompt variant and add the wine description to it\n",
    "    #we also add a few examples of what we want the model to do to help it understand the task\n",
    "    messages = [\n",
    "       {\"role\": \"system\", \"content\": f\"\"\"{prompt} Queries must sound like a real person searching online. No more than 2 descriptors. Avoid full sentences or wine jargon.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        Wine: \"Flavors of blackberry, plum, leather, and dried herbs with firm tannins and a smoky finish.\"  \n",
    "        Query: \"bold wine with dark fruit.\"\n",
    "\n",
    "        Wine: \"Aromas of green apple, lime zest, and flinty minerality with bright acidity.\"  \n",
    "        Query: \"Crisp white with citrus notes\"\n",
    "        \n",
    "        Wine: \"Bright citrus and minerality with crisp acidity.\"\n",
    "        Query: \"Zesty white with citrus\"\n",
    "\n",
    "        Wine: \"Ripe red cherry, baking spice, and light oak tannins.\"\n",
    "        Query: \"Fruity red with spice\"\n",
    "\n",
    "        Now generate a query for this wine:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Write a natural user-style query for the following wine:\\n{wine_desc}\\nQuery:\"}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, return_attention_mask=True).to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.8,\n",
    "        top_p=0.85,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    #prune off exess description and \"\"\n",
    "    response = response.replace('\"', \"\")\n",
    "    response = trim_query(response)\n",
    "    \n",
    "    #same as the random prompt variants we used to generate the query we also have a list of random restructuring patterns\n",
    "    #this is to help keep the data more diverse and prevent over fitting in the bert model\n",
    "    restructure_patterns = [\n",
    "    lambda desc: f\"{desc}\",  \n",
    "    lambda desc: f\"A {desc} wine\", \n",
    "    lambda desc: f\"{desc}, perfect with food\",  \n",
    "    lambda desc: f\"Looking for a {desc} wine\",  \n",
    "    lambda desc: f\"{desc} ideal with dinner\",  \n",
    "    lambda desc: f\"somthing that is {desc}\",\n",
    "    lambda desc: f\"Wine with {desc}\",\n",
    "    lambda desc: f\"Searching for {desc}\",\n",
    "    lambda desc: f\"Wine that is {desc}\",\n",
    "    ]\n",
    "    \n",
    "    # Randomly select a restructuring pattern\n",
    "    restructure = random.choice(restructure_patterns)\n",
    "    # Apply the restructuring\n",
    "    response = restructure(response)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model code has been set up we can now make the code to generate the pairs. Because we are using a subset of our entire dataset we want to use some stratification to ensure we get a good subset. In this case we are using \n",
    "- price buckets\n",
    "- country of origin\n",
    "\n",
    "\n",
    "note: if you don't have a powerful gpu you can skip this step and use the pre-generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate price buckets for stratified sampling\n",
    "def price_bucket(price):\n",
    "    try:\n",
    "        price = float(price)\n",
    "        if price < 15:\n",
    "            return \"low\"\n",
    "        elif price < 30:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"high\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "#Proportional stratified sampling to ensure we have a balanced dataset\n",
    "def stratified_samples(df, target_total=3000):\n",
    "    df = df.dropna(subset=[\"description\", \"country\", \"price\"]).copy()\n",
    "    df[\"price_bucket\"] = df[\"price\"].apply(price_bucket)\n",
    "    df = df.dropna(subset=[\"price_bucket\"])\n",
    "\n",
    "    group_keys = [\"country\", \"price_bucket\"]\n",
    "    grouped = df.groupby(group_keys)\n",
    "\n",
    "    # Get group sizes\n",
    "    group_sizes = grouped.size()\n",
    "\n",
    "    # Calculate proportional samples per group\n",
    "    total_available = group_sizes.sum()\n",
    "    group_targets = (group_sizes / total_available * target_total).round().astype(int)\n",
    "\n",
    "    # Sample from each group\n",
    "    sampled_dfs = []\n",
    "    for group, size in group_targets.items():\n",
    "        group_df = grouped.get_group(group)\n",
    "        sampled = group_df.sample(min(size, len(group_df)), random_state=42)\n",
    "        sampled_dfs.append(sampled)\n",
    "\n",
    "    stratified_df = pd.concat(sampled_dfs).reset_index(drop=True)\n",
    "    return stratified_df\n",
    "\n",
    "\n",
    "def generate_stratified_queries(df, target_total=3000, output_path=\"fixed_stratified_pairs.jsonl\"):\n",
    "    stratified_df = stratified_samples(df, target_total)\n",
    "\n",
    "    #save the stratified samples to a jsonl file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in tqdm(stratified_df.iterrows(), total=len(stratified_df), desc=\"Generating queries\"):\n",
    "            desc = row[\"description\"]\n",
    "            try:\n",
    "                query = generate_sample_query(desc)\n",
    "                item = {\n",
    "                    \"texts\": [query, desc],\n",
    "                    \"label\": 1.0\n",
    "                }\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"[x] Skipping row due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Saved {len(stratified_df)} examples to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the positive examples and generate negative examples by shuffling the descriptions around\n",
    "def generate_negative_pairs_from_positive(positive_path, output_path=\"negative_pairs.jsonl\"):\n",
    "    # Load the positive examples\n",
    "    with open(positive_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        positive_data = [json.loads(line) for line in f]\n",
    "\n",
    "    queries = [item[\"texts\"][0] for item in positive_data]\n",
    "    descriptions = [item[\"texts\"][1] for item in positive_data]\n",
    "\n",
    "    # Shuffle descriptions until none are in original positions\n",
    "    shuffled_descriptions = descriptions.copy()\n",
    "    attempts = 0\n",
    "    while any(qd == sd for qd, sd in zip(descriptions, shuffled_descriptions)) and attempts < 10:\n",
    "        random.shuffle(shuffled_descriptions)\n",
    "        attempts += 1\n",
    "\n",
    "    # Build negative examples\n",
    "    negative_examples = []\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for query, wrong_desc in tqdm(zip(queries, shuffled_descriptions), total=len(queries), desc=\"Generating negative pairs\"):\n",
    "            example = {\n",
    "                \"texts\": [query, wrong_desc],\n",
    "                \"label\": 0.0\n",
    "            }\n",
    "            f_out.write(json.dumps(example) + \"\\n\")\n",
    "            negative_examples.append(InputExample(texts=[query, wrong_desc], label=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have code that selects and generates queries it is time to run it if you don't have the gpu to run this model just skip the first code cell and load in one of the pre-generated data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the stratified queries and data pairs\n",
    "generate_stratified_queries(df, target_total=100, output_path=\"stratified_positive_pairs_mini.jsonl\")\n",
    "#generate the negative pairs from the positive pairs\n",
    "generate_negative_pairs_from_positive(\"stratified_positive_pairs_mini.jsonl\", output_path=\"stratified_negative_pairs_mini.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pre generated data\n",
    "def load_examples_from_jsonl(path):\n",
    "    examples = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            examples.append(InputExample(texts=item[\"texts\"], label=float(item[\"label\"])))\n",
    "    return examples\n",
    "\n",
    "positive_examples = load_examples_from_jsonl(\"stratified_positive_pairs_mini.jsonl\")\n",
    "negative_examples = load_examples_from_jsonl(\"stratified_negative_pairs_mini.jsonl\")\n",
    "\n",
    "all_examples = positive_examples + negative_examples\n",
    "random.shuffle(all_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have our generated data we want to check how diverse it is ie how different the queries are if the value is too high > .7 this would result in the bert model being to focused on one type of query and not perform well\n",
    "if its to low < .3 then the data we made is meaningless and no trend can be found, in general use data from things like google searches has around a .4 to .6 diversity so that is what we want to get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the bert based sentence transformer model as we need its embedding function to calculate the similarity scores\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all user queries\n",
    "user_queries = []\n",
    "for example in positive_examples:\n",
    "    user_queries.append(example.texts[0])\n",
    "\n",
    "#get the embeddings for the user queries\n",
    "embeddings = model.encode(user_queries, convert_to_tensor=True)\n",
    "\n",
    "#calculate the cosine similarity between the embeddings\n",
    "sim_matrix = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "upper_tri_sim = sim_matrix.triu(diagonal=1)\n",
    "print(1.0 - upper_tri_sim[upper_tri_sim != 0].mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 | Train the Model\n",
    "now that we created our complete data set using prompt engineering we can now began training the bert based model in order to better select wine recommendations based off of taste profile descriptors\n",
    "\n",
    "\n",
    "Before we do that we will first take a baseline so we can see how well the model performs before and after. in-order to do this we will pre compute all embeddings and for each query we will calculate the similarity score the wine descriptions to decide which one the model would recommend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExampleEmbedding:\n",
    "    \"\"\"Structure for one input example with texts, the label and a unique id\"\"\"\n",
    "\n",
    "    def __init__(self, guid: str = \"\", texts: list[str] = None, label: int | float = 0, prompt: torch.Tensor = None, desc: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Creates one InputExample with the given texts, guid and label\n",
    "\n",
    "        Args:\n",
    "            guid: id for the example\n",
    "            texts: the texts for the example\n",
    "            label: the label for the example\n",
    "            prompt: the prompt embedding for the example\n",
    "            desc: the description embedding for the example\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.texts = texts\n",
    "        self.label = label\n",
    "        self.prompt = prompt\n",
    "        self.desc = desc\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<InputExample> label: {}, texts: {}, prompt: {}, desc: {}\".format(str(self.label), \"; \".join(self.texts), str(self.prompt), str(self.desc))\n",
    "    \n",
    "# Pre-encode pairs\n",
    "def encode_all(query_desc_pairs):\n",
    "    embeddings = []\n",
    "\n",
    "    for pair_idx in range(len(query_desc_pairs)):\n",
    "        query = query_desc_pairs[pair_idx].texts[0]\n",
    "        desc = query_desc_pairs[pair_idx].texts[1]\n",
    "\n",
    "        # Encode and ensure everything is on the same device\n",
    "        query_emb = model.encode(query, convert_to_tensor=True).to(device)\n",
    "        desc_emb = model.encode(desc, convert_to_tensor=True).to(device)\n",
    "\n",
    "        # Store the embeddings in the pair\n",
    "        embeddings.append(InputExampleEmbedding(\n",
    "            texts=[query, desc],\n",
    "            label=query_desc_pairs[pair_idx].label,\n",
    "            prompt=query_emb,\n",
    "            desc=desc_emb\n",
    "        ))\n",
    "    return embeddings\n",
    "\n",
    "# Save the encoded pairs to a file for future use\n",
    "def save_encoded_pairs(encoded_pairs, file_path=\"encoded_pairs.jsonl\"):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for pair in encoded_pairs:\n",
    "            record = {\n",
    "                'texts': pair.texts,\n",
    "                'label': pair.label,\n",
    "                'query_embedding': pair.prompt.cpu().tolist(),\n",
    "                'desc_embedding': pair.desc.cpu().tolist()\n",
    "            }\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "# Load saved pairs from a file\n",
    "def load_encoded_pairs(file_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loaded_data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "\n",
    "            # Convert embeddings back to tensors\n",
    "            query_emb = torch.tensor(record['query_embedding'], device=device)\n",
    "            desc_emb = torch.tensor(record['desc_embedding'], device=device)\n",
    "            \n",
    "            # Reconstruct the structure\n",
    "            loaded_data.append(InputExampleEmbedding(\n",
    "                texts=record['texts'],\n",
    "                label=record['label'],\n",
    "                prompt=query_emb,\n",
    "                desc=desc_emb\n",
    "            ))\n",
    "    return loaded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have the code to generate the embeddings lets go ahead and do that, as before if your gpu cant handel this feel free to use the provided saved embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-encode the embeddings for the training data pairs\n",
    "encoded_pairs = encode_all(positive_examples[:100])\n",
    "# Save the encoded pairs to a file\n",
    "save_encoded_pairs(encoded_pairs, file_path=\"pre_train_encoded_pairs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the encoded pairs from the file\n",
    "encoded_pairs = load_encoded_pairs(file_path=\"pre_train_encoded_pairs.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that the embeddings have been pre-computed we can use them to test the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline accuracy\n",
    "def compute_accuracy_pre_emb(encoded_pairs, candidates_per_query=10):\n",
    "    correct = 0\n",
    "    top_5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for pair in encoded_pairs:\n",
    "        correct_desc = pair.texts[1]\n",
    "        query_emb = pair.prompt\n",
    "        correct_desc_emb = pair.desc\n",
    "\n",
    "        # Get negative samples\n",
    "        incorrect = random.sample([[p.texts[1], p.desc] for p in encoded_pairs if p.texts[1] != correct_desc], k=candidates_per_query - 1)\n",
    "        candidates = [[correct_desc, correct_desc_emb]] + incorrect\n",
    "\n",
    "        cand_embs = [pair[1] for pair in candidates]\n",
    "        candidates = [pair[0] for pair in candidates]\n",
    "\n",
    "        # Compute similarity\n",
    "        cand_embs_tensor = torch.stack(cand_embs)\n",
    "        query_emb_tensor = query_emb.unsqueeze(0)\n",
    "        sims = util.cos_sim(query_emb_tensor, cand_embs_tensor)[0]\n",
    "        correct_idx = candidates.index(correct_desc)\n",
    "        top5_indices = torch.topk(sims, k=5).indices.tolist()\n",
    "\n",
    "        if top5_indices[0] == correct_idx:\n",
    "            correct += 1\n",
    "        if correct_idx in top5_indices:\n",
    "            top_5_correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total, top_5_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feel free to change the number of candidates per query to get a more accurate accuracy score at the cost of computation time\n",
    "perfect, top5 = compute_accuracy_pre_emb(encoded_pairs, candidates_per_query=10)\n",
    "print(f\"Perfect accuracy: {perfect:.4f}\")\n",
    "print(f\"Top 5 accuracy: {top5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final now that we have a baseline its time to train the model lucky for use the sentence transformer library has a training function built in so fine tuning is easy from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(all_examples, shuffle=True, batch_size=16)  # Adjust batch size if needed\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=5,\n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True,\n",
    "    output_path=\"fine-tuned-minilm\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at model results, if they are not the best don't worry we have a fine tuned version that can be downloaded and played with later to see its results as before we will pre compute embendings to use for accuracy training and provide pre done ones if you don't have a strong enough gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-encode the embeddings for the training data pairs\n",
    "encoded_pairs = encode_all(positive_examples[:100])\n",
    "# Save the encoded pairs to a file\n",
    "save_encoded_pairs(encoded_pairs, file_path=\"post_train_encoded_pairs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the encoded pairs from the file\n",
    "encoded_pairs = load_encoded_pairs(file_path=\"post_train_encoded_pairs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feel free to change the number of candidates per query to get a more accurate accuracy score at the cost of computation time\n",
    "perfect, top5 = compute_accuracy_pre_emb(encoded_pairs, candidates_per_query=10)\n",
    "print(f\"Perfect accuracy: {perfect:.4f}\")\n",
    "print(f\"Top 5 accuracy: {top5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fine-tuned-minilm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 | play with the model\n",
    "now that the model is working you can feel free to test it out, just change up the query as you see fit and look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in pre trained model from the cloud\n",
    "model = SentenceTransformer(\"SpencerCreveling99/fine-tuned-minilm-wine\")\n",
    "#load this models embeddings\n",
    "encoded_pairs = load_encoded_pairs(file_path=\"post_train_sample_encoded_pairs.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I'm looking for a bold red wine with black fruit and spice.\"\n",
    "descriptions = df[\"description\"].dropna().tolist()\n",
    "\n",
    "query_emb = model.encode(query, convert_to_tensor=True).to(device)\n",
    "desc_embs = torch.stack([example.desc for example in encoded_pairs]).to(device)\n",
    "\n",
    "#Compute cosine similarities\n",
    "cosine_scores = util.cos_sim(query_emb, desc_embs)[0]\n",
    "\n",
    "#Get top N results\n",
    "top_k = 5\n",
    "top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "#Show results\n",
    "print(\"\\nTop matching wines:\\n\" + \"-\" * 40)\n",
    "for score, idx in zip(top_results.values, top_results.indices):\n",
    "    print(f\"Score: {score.item():.4f}\")\n",
    "    print(f\"Wine Description: {descriptions[idx]}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
