{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import is_torch_sdpa_available \n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "model = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2065616/3176498914.py:4: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.11).\n"
     ]
    }
   ],
   "source": [
    "file_path = \"winemag-data-130k-v2.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"zynicide/wine-reviews\",\n",
    "  file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "if is_torch_sdpa_available():\n",
    "    torch._dynamo.config.suppress_errors = True  # optional\n",
    "    torch.backends.cuda.enable_flash_sdp(False)\n",
    "    torch.backends.cuda.enable_math_sdp(True)\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.sliding_window = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def trim_query(text, max_parts=3):\n",
    "    # Lowercase the text for normalization\n",
    "    text = text.strip()\n",
    "\n",
    "    # Split by common descriptor joiners\n",
    "    parts = re.split(r'\\s*(?:,| and | with |\\. )\\s*', text)\n",
    "\n",
    "    # Keep only the first few parts\n",
    "    trimmed_parts = parts[:max_parts]\n",
    "\n",
    "    # Re-join them with commas or 'and' for flow\n",
    "    if len(trimmed_parts) > 1:\n",
    "        result = ', '.join(trimmed_parts[:-1]) + ' and ' + trimmed_parts[-1]\n",
    "    else:\n",
    "        result = trimmed_parts[0]\n",
    "\n",
    "    return result.strip().capitalize()\n",
    "\n",
    "def generate_sample_query(wine_desc):\n",
    "    \n",
    "    prompt_variants = [\n",
    "    \"\"\"You are a helpful assistant that creates short, human-like wine search queries from wine descriptions. The queries should be realistic, not too detailed, and focus on 1–3 key aspects of the wine.\"\"\",\n",
    "\n",
    "    \"\"\"You are an assistant that turns wine descriptions into simple, casual search phrases that a real person might use. Do not list everything—just highlight a few main traits.\"\"\",\n",
    "\n",
    "    \"\"\"You generate concise, informal wine search queries based on tasting notes. Keep it brief and include only the most noticeable characteristics.\"\"\",\n",
    "\n",
    "    \"\"\"You're helping wine lovers describe wines in simple search terms. Summarize the wine's essence in 1–2 quick flavor notes or attributes.\"\"\",\n",
    "\n",
    "    \"\"\"You generate short, realistic search queries from wine descriptions. Keep the language natural and intuitive. Mention just a couple key traits.\"\"\",\n",
    "\n",
    "    \"\"\"Your job is to turn wine descriptions into casual search-style queries using only the most important descriptors. Avoid over-detailing or repetition.\"\"\",\n",
    "\n",
    "    \"\"\"You create short user-style search queries based on wine descriptions. Stick to 1–3 high-impact descriptors. Be realistic and natural, not formal.\"\"\",\n",
    "\n",
    "    \"\"\"You are generating imperfect but useful search queries from wine descriptions. The goal is to sound like a real wine shopper, not list every flavor.\"\"\",\n",
    "\n",
    "    \"\"\"Turn the wine descriptions into realistic, search-friendly phrases a person might type. Include only standout flavors or textures.\"\"\",\n",
    "\n",
    "    \"\"\"You're writing very short, human-friendly search terms based on full wine descriptions. Focus on the most prominent flavors or textures.\"\"\"\n",
    "    ]   \n",
    "\n",
    "    # Select a random prompt variant\n",
    "    prompt = prompt_variants[random.randint(0, len(prompt_variants) - 1)]\n",
    "    \n",
    "    messages = [\n",
    "       {\"role\": \"system\", \"content\": f\"\"\"{prompt} Queries must sound like a real person searching online. No more than 2 descriptors. Avoid full sentences or wine jargon.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        Wine: \"Flavors of blackberry, plum, leather, and dried herbs with firm tannins and a smoky finish.\"  \n",
    "        Query: \"bold wine with dark fruit.\"\n",
    "\n",
    "        Wine: \"Aromas of green apple, lime zest, and flinty minerality with bright acidity.\"  \n",
    "        Query: \"Crisp white with citrus notes\"\n",
    "        \n",
    "        Wine: \"Bright citrus and minerality with crisp acidity.\"\n",
    "        Query: \"Zesty white with citrus\"\n",
    "\n",
    "        Wine: \"Ripe red cherry, baking spice, and light oak tannins.\"\n",
    "        Query: \"Fruity red with spice\"\n",
    "\n",
    "        Now generate a query for this wine:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Write a natural user-style query for the following wine:\\n{wine_desc}\\nQuery:\"}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, return_attention_mask=True).to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.8,\n",
    "        top_p=0.85,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    #prune off exess description and \"\"\n",
    "    response = response.replace('\"', \"\")\n",
    "    response = trim_query(response)\n",
    "    \n",
    "    restructure_patterns = [\n",
    "    lambda desc: f\"{desc}\",  \n",
    "    lambda desc: f\"A {desc} wine\", \n",
    "    lambda desc: f\"{desc}, perfect with food\",  \n",
    "    lambda desc: f\"Looking for a {desc} wine\",  \n",
    "    lambda desc: f\"{desc} ideal with dinner\",  \n",
    "    lambda desc: f\"somthing that is {desc}\",\n",
    "    lambda desc: f\"Wine with {desc}\",\n",
    "    lambda desc: f\"Searching for {desc}\",\n",
    "    lambda desc: f\"Wine that is {desc}\",\n",
    "    ]\n",
    "    \n",
    "    # Randomly select a restructuring pattern\n",
    "    restructure = random.choice(restructure_patterns)\n",
    "    # Apply the restructuring\n",
    "    response = restructure(response)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_positive_pairs(\n",
    "    df: pd.DataFrame,\n",
    "    num_samples=1000,\n",
    "    output_file=\"positive_pairs.jsonl\"\n",
    "):\n",
    "    # Step 1: Drop NaNs and prepare repeat logic\n",
    "    df = df.dropna(subset=[\"description\"]).reset_index(drop=True)\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"No valid wine descriptions found in the dataset.\")\n",
    "\n",
    "    if num_samples > len(df):\n",
    "        reps = math.ceil(num_samples / len(df))\n",
    "        df = pd.concat([df] * reps, ignore_index=True).sample(n=num_samples).reset_index(drop=True)\n",
    "    else:\n",
    "        df = df.sample(n=num_samples).reset_index(drop=True)\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, row in tqdm(df.iterrows(), total=num_samples, desc=\"Generating positive pairs\"):\n",
    "            wine_desc = row[\"description\"]\n",
    "            try:\n",
    "                user_query = generate_sample_query(wine_desc)\n",
    "                example = InputExample(texts=[user_query, wine_desc], label=1.0)\n",
    "                examples.append(example)\n",
    "\n",
    "                f.write(json.dumps({\n",
    "                    \"texts\": [user_query, wine_desc],\n",
    "                    \"label\": 1.0\n",
    "                }) + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"[x] Skipped row {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_pairs_from_positive(positive_path, output_path=\"negative_pairs.jsonl\"):\n",
    "    # Load the positive examples\n",
    "    with open(positive_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        positive_data = [json.loads(line) for line in f]\n",
    "\n",
    "    queries = [item[\"texts\"][0] for item in positive_data]\n",
    "    descriptions = [item[\"texts\"][1] for item in positive_data]\n",
    "\n",
    "    # Shuffle descriptions until none are in original positions\n",
    "    shuffled_descriptions = descriptions.copy()\n",
    "    attempts = 0\n",
    "    while any(qd == sd for qd, sd in zip(descriptions, shuffled_descriptions)) and attempts < 10:\n",
    "        random.shuffle(shuffled_descriptions)\n",
    "        attempts += 1\n",
    "\n",
    "    if attempts == 10:\n",
    "        print(\"⚠️ Could not perfectly avoid overlaps after 10 tries. Still writing.\")\n",
    "\n",
    "    # Build negative examples\n",
    "    negative_examples = []\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for query, wrong_desc in tqdm(zip(queries, shuffled_descriptions), total=len(queries), desc=\"Generating negative pairs\"):\n",
    "            example = {\n",
    "                \"texts\": [query, wrong_desc],\n",
    "                \"label\": 0.0\n",
    "            }\n",
    "            f_out.write(json.dumps(example) + \"\\n\")\n",
    "            negative_examples.append(InputExample(texts=[query, wrong_desc], label=0.0))\n",
    "\n",
    "    return negative_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_test_pairs = generate_positive_pairs(\n",
    "    df,\n",
    "    num_samples=6000,\n",
    "    output_file=\"positive_pairs.jsonl\"\n",
    ")\n",
    "\n",
    "negative_test_pairs = generate_negative_pairs_from_positive(\n",
    "    positive_path=\"positive_pairs.jsonl\",\n",
    "    output_path=\"negative_pairs.jsonl\"\n",
    ")\n",
    "\n",
    "#print 5 positive and negative pairs\n",
    "def print_pairs(pairs, label):\n",
    "    print(f\"\\n{label} pairs:\")\n",
    "    for i, pair in enumerate(pairs[:5]):\n",
    "        query, desc = pair.texts\n",
    "        print(f\"Pair {i+1}:\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Description: {desc}\")\n",
    "        print(\"-\" * 60)\n",
    "print_pairs(positive_test_pairs, \"Positive\")\n",
    "print_pairs(negative_test_pairs, \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔢 Price bucket function\n",
    "def price_bucket(price):\n",
    "    try:\n",
    "        price = float(price)\n",
    "        if price < 15:\n",
    "            return \"low\"\n",
    "        elif price < 30:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"high\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# 🧪 Proportional stratified sampling\n",
    "def stratified_sample_fixed_total(df, target_total=3000):\n",
    "    df = df.dropna(subset=[\"description\", \"country\", \"price\"]).copy()\n",
    "    df[\"price_bucket\"] = df[\"price\"].apply(price_bucket)\n",
    "    df = df.dropna(subset=[\"price_bucket\"])\n",
    "\n",
    "    group_keys = [\"country\", \"price_bucket\"]\n",
    "    grouped = df.groupby(group_keys)\n",
    "\n",
    "    # Get group sizes\n",
    "    group_sizes = grouped.size()\n",
    "    total_groups = len(group_sizes)\n",
    "\n",
    "    # Calculate proportional samples per group\n",
    "    total_available = group_sizes.sum()\n",
    "    group_targets = (group_sizes / total_available * target_total).round().astype(int)\n",
    "\n",
    "    # Sample from each group\n",
    "    sampled_dfs = []\n",
    "    for group, size in group_targets.items():\n",
    "        group_df = grouped.get_group(group)\n",
    "        sampled = group_df.sample(min(size, len(group_df)), random_state=42)\n",
    "        sampled_dfs.append(sampled)\n",
    "\n",
    "    stratified_df = pd.concat(sampled_dfs).reset_index(drop=True)\n",
    "    return stratified_df\n",
    "\n",
    "def generate_stratified_queries_fixed_total(df, target_total=3000, output_path=\"fixed_stratified_pairs.jsonl\"):\n",
    "    stratified_df = stratified_sample_fixed_total(df, target_total)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in tqdm(stratified_df.iterrows(), total=len(stratified_df), desc=\"Generating queries\"):\n",
    "            desc = row[\"description\"]\n",
    "            try:\n",
    "                query = generate_sample_query(desc)\n",
    "                item = {\n",
    "                    \"texts\": [query, desc],\n",
    "                    \"label\": 1.0\n",
    "                }\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"[x] Skipping row due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"\\n✅ Saved {len(stratified_df)} examples to: {output_path}\")\n",
    "\n",
    "generate_stratified_queries_fixed_total(\n",
    "    df,\n",
    "    target_total=20000,\n",
    "    output_path=\"fixed_stratified_pairs.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate negatives from fixed_stratified_pairs.jsonl\n",
    "negative_test_pairs = generate_negative_pairs_from_positive(\n",
    "    positive_path=\"fixed_positive_stratified_pairs.jsonl\",\n",
    "    output_path=\"fixed_negative_stratified_pairs.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diversity check to make sure we dont have fake queries that are too similar or too different \n",
    "\n",
    "#read in positinve and negative pairs\n",
    "positive_test_pairs = []\n",
    "with open(\"fixed_positive_stratified_pairs.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        positive_test_pairs.append(InputExample(**json.loads(line)))\n",
    "negative_test_pairs = []\n",
    "with open(\"fixed_negative_stratified_pairs.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        negative_test_pairs.append(InputExample(**json.loads(line)))\n",
    "        \n",
    "        \n",
    "combined_pairs = positive_test_pairs + negative_test_pairs\n",
    "#pull out just user queries\n",
    "user_queries = []\n",
    "for example in combined_pairs:\n",
    "    user_queries.append(example.texts[0])\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(user_queries, convert_to_tensor=True)\n",
    "sim_matrix = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "upper_tri_sim = sim_matrix.triu(diagonal=1)\n",
    "print(1.0 - upper_tri_sim[upper_tri_sim != 0].mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples_from_jsonl(path):\n",
    "    examples = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            examples.append(InputExample(texts=item[\"texts\"], label=float(item[\"label\"])))\n",
    "    return examples\n",
    "\n",
    "positive_examples = load_examples_from_jsonl(\"fixed_positive_stratified_pairs.jsonl\")\n",
    "negative_examples = load_examples_from_jsonl(\"fixed_negative_stratified_pairs.jsonl\")\n",
    "\n",
    "# Combine and shuffle\n",
    "all_examples = positive_examples + negative_examples\n",
    "random.shuffle(all_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(all_examples, shuffle=True, batch_size=16)  # Adjust batch size if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "# Use CosineSimilarityLoss (good for binary positive/negative matching)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4564, 0.6796)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get basline accuracy\n",
    "def compute_accuracy(model, query_desc_pairs, candidates_per_query=10):\n",
    "    correct = 0\n",
    "    top_5_correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for pair in query_desc_pairs:\n",
    "        query = pair.texts[0]\n",
    "        correct_desc = pair.texts[1]\n",
    "\n",
    "        # Get negative samples\n",
    "        incorrect = random.sample([p.texts[1] for p in query_desc_pairs if p.texts[1] != correct_desc], k=candidates_per_query - 1)\n",
    "        candidates = [correct_desc] + incorrect\n",
    "        random.shuffle(candidates)\n",
    "\n",
    "        # Encode and ensure everything is on the same device\n",
    "        query_emb = model.encode(query, convert_to_tensor=True).to(device)\n",
    "        cand_embs = model.encode(candidates, convert_to_tensor=True).to(device)\n",
    "\n",
    "        # Compute similarity\n",
    "        sims = util.cos_sim(query_emb, cand_embs)[0]\n",
    "        correct_idx = candidates.index(correct_desc)\n",
    "        top5_indices = torch.topk(sims, k=5).indices.tolist()\n",
    "\n",
    "        if top5_indices[0] == correct_idx:\n",
    "            correct += 1\n",
    "        if correct_idx in top5_indices:\n",
    "            top_5_correct += 1\n",
    "        total += 1\n",
    "        \n",
    "\n",
    "    return correct / total, top_5_correct / total\n",
    "\n",
    "\n",
    "\n",
    "compute_accuracy(model, positive_examples[:5000], candidates_per_query=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3b27d3e27947ccba900fd850f10973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 09:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.044600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=5,\n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True,\n",
    "    output_path=\"fine-tuned-minilm\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect accuracy: 0.8112\n",
      "Top 5 accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "perfect, top5 = compute_accuracy(model, positive_examples[:5000], candidates_per_query=50)\n",
    "print(f\"Perfect accuracy: {perfect:.4f}\")\n",
    "print(f\"Top 5 accuracy: {top5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fine-tuned-minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top matching wines:\n",
      "----------------------------------------\n",
      "Score: 0.9609\n",
      "Wine Description: A slam-dunk to have with tonight’s lasagna. It’s loaded with black fruit and spices, both on the nose and on the palate. Medium-bodied and well balanced, the finish is delightfully long.\n",
      "----------------------------------------\n",
      "Score: 0.9605\n",
      "Wine Description: This ripe and spicy wine from the Dop family's 27 acres offers layers of black fruits. It has a concentrated texture and a firm structure that is juicy while dense. With this richness, the wine will develop into a smooth and generous wine. Drink from 2019.\n",
      "----------------------------------------\n",
      "Score: 0.9593\n",
      "Wine Description: Big, bold and spicy, this wine is ripe with black fruit. Tannins and a rich texture confirm its upfront style. Likely to take its time to develop, it will be ready to drink from 2020.\n",
      "----------------------------------------\n",
      "Score: 0.9571\n",
      "Wine Description: This wine is rich, spicy and full of black fruit. With solid tannins alongside its juicy black currant flavors, it's firm and will age well. Its density and richness are developing and will be best from 2020.\n",
      "----------------------------------------\n",
      "Score: 0.9527\n",
      "Wine Description: Aromas of black fruits, integrated spices, licorice and tobacco are sturdy and almost heady. This feels layered, with structurally sound, slightly scraping tannins. Flavors of spicy cherry and cassis finish ripe, loamy and with a note of prune. Drink through 2020.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"I'm looking for a bold red wine with black fruit and spice.\"\n",
    "descriptions = df[\"description\"].dropna().tolist()\n",
    "\n",
    "query_emb = model.encode(query, convert_to_tensor=True).to(device)\n",
    "desc_embs = model.encode(descriptions, batch_size=32, convert_to_tensor=True).to(device)\n",
    "\n",
    "# ✅ Compute cosine similarities\n",
    "cosine_scores = util.cos_sim(query_emb, desc_embs)[0]\n",
    "\n",
    "# ✅ Get top N results\n",
    "top_k = 5\n",
    "top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "# ✅ Show results\n",
    "print(\"\\nTop matching wines:\\n\" + \"-\" * 40)\n",
    "for score, idx in zip(top_results.values, top_results.indices):\n",
    "    print(f\"Score: {score.item():.4f}\")\n",
    "    print(f\"Wine Description: {descriptions[idx]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload moderl to hub\n",
    "from kagglehub import KaggleHub\n",
    "from huggingface_hub import HfApi, upload_folder\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(\n",
    "    repo_id=\"fine-tuned-minilm-wine\",\n",
    "    private=False\n",
    ")\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=\"SpencerCreveling99/fine-tuned-minilm-wine\",\n",
    "    folder_path=\"./fine-tuned-minilm\",\n",
    "    commit_message=\"Initial commit\",\n",
    "    repo_type=\".\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
