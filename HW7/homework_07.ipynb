{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSWEcS2XKgzi"
      },
      "source": [
        "# HW 7 (10 pt)\n",
        "### Parameter Efficient Fine-Tuning\n",
        "In this notebook, you're gonna fine-tune large language models within limited GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T01:32:25.834446Z",
          "iopub.status.busy": "2025-03-16T01:32:25.834037Z",
          "iopub.status.idle": "2025-03-16T01:32:50.029616Z",
          "shell.execute_reply": "2025-03-16T01:32:50.028904Z",
          "shell.execute_reply.started": "2025-03-16T01:32:25.834402Z"
        },
        "id": "7xeRF_hSKgzs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#pip install --quiet --upgrade transformers bitsandbytes accelerate sentencepiece optimum auto-gptq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import BitsAndBytesConfig\n",
        "from tqdm.auto import tqdm, trange\n",
        "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # use 'cuda' for any GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T01:32:50.031084Z",
          "iopub.status.busy": "2025-03-16T01:32:50.030706Z",
          "iopub.status.idle": "2025-03-16T01:35:09.589061Z",
          "shell.execute_reply": "2025-03-16T01:35:09.588122Z",
          "shell.execute_reply.started": "2025-03-16T01:32:50.031063Z"
        },
        "id": "VMzFwx29Kgzu",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/u28/spencer/anaconda3/envs/llm2/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/users/u28/spencer/anaconda3/envs/llm2/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/users/u28/spencer/anaconda3/envs/llm2/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa1ebf9e25ee429d8b91eb2ce9f02adc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/u28/spencer/anaconda3/envs/llm2/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = 'Enoch/llama-7b-hf'\n",
        "\n",
        "# loading Llama tokenizer ...\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ... and the model itself\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True, # use device_map='auto' for multipl GPUs\n",
        "    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
        "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
        "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgspB2JwSIS2"
      },
      "source": [
        "### Prompt tuning: the story of a fox (2 pts)\n",
        "\n",
        "![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-15T21:03:12.608021Z",
          "iopub.status.busy": "2025-03-15T21:03:12.60771Z",
          "iopub.status.idle": "2025-03-15T21:03:16.080822Z",
          "shell.execute_reply": "2025-03-15T21:03:16.080011Z",
          "shell.execute_reply.started": "2025-03-15T21:03:12.607997Z"
        },
        "id": "H13pYFRxQi4U",
        "outputId": "597e1af9-399a-41ab-8d8d-9c1c216d906c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: <s>A quick brown fox jumps over the lazy dog.\n",
            "A quick\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVhZACT6SgLq"
      },
      "source": [
        "What a blatant lie! This particular fox assures you that it did not, in fact, jump over the lazy dog. No, sir! The fox was merely minding its own business. __Your task is to train the model to tell the truth: no dog was jumped over today.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-15T01:49:11.67137Z",
          "iopub.status.busy": "2025-03-15T01:49:11.671042Z",
          "iopub.status.idle": "2025-03-15T01:49:12.352048Z",
          "shell.execute_reply": "2025-03-15T01:49:12.351244Z",
          "shell.execute_reply.started": "2025-03-15T01:49:11.671349Z"
        },
        "id": "_r6UVDl4NEua",
        "outputId": "67ab27e0-af96-41c7-f0a9-db92e842cd80",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: tensor(3.0729, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "outputs = model(**batch)\n",
        "\n",
        "next_word_logits = outputs.logits[:, :-1]\n",
        "true_next_tokens = batch['input_ids'][:, 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "\n",
        "print(\"Loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amvNufS8WXa0"
      },
      "source": [
        "Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n",
        "\n",
        "![img](https://i.imgur.com/VwNNKnb.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEkoFNdlshv_"
      },
      "source": [
        "### Using HuggingFace PEFT (5 points)\n",
        "\n",
        "[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various **p**arameter **e**fficient **f**ine-**t**uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-15T02:13:01.295016Z",
          "iopub.status.busy": "2025-03-15T02:13:01.294666Z",
          "iopub.status.idle": "2025-03-15T02:13:01.316097Z",
          "shell.execute_reply": "2025-03-15T02:13:01.315189Z",
          "shell.execute_reply.started": "2025-03-15T02:13:01.29499Z"
        },
        "id": "mqEEpZm2Q4UC",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "2b760d0e-ac1e-4580-9472-997d1275385d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 65536\n",
            "Total parameters (excluding quantization): 3500478464\n"
          ]
        }
      ],
      "source": [
        "import peft\n",
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n",
        "\n",
        "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n",
        "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n",
        "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-IlPcRhoM8e"
      },
      "source": [
        "# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n",
        "# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n",
        "# Finally, generate the sentence to make sure that the model learned the truth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfeUMracocqG"
      },
      "source": [
        "You may need to use the following parameters in optimizor:\n",
        "`Adam([param for name, param in model.named_parameters() if param.requires_grad], lr=0.01)`\n",
        "\n",
        "For multiple GPU usage you can use `model = torch.nn.DataParallel(model)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-15T21:03:36.700415Z",
          "iopub.status.busy": "2025-03-15T21:03:36.700032Z",
          "iopub.status.idle": "2025-03-15T21:03:36.706634Z",
          "shell.execute_reply": "2025-03-15T21:03:36.705329Z",
          "shell.execute_reply.started": "2025-03-15T21:03:36.700382Z"
        },
        "id": "mCRGv_u3xZkI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Set up optimizer to only update PEFT parameters\n",
        "from torch.optim import Adam\n",
        "optimizer = Adam([p for n, p in model.named_parameters() if p.requires_grad], lr=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-15T21:03:49.971306Z",
          "iopub.status.busy": "2025-03-15T21:03:49.970851Z",
          "iopub.status.idle": "2025-03-15T21:03:49.977123Z",
          "shell.execute_reply": "2025-03-15T21:03:49.97617Z",
          "shell.execute_reply.started": "2025-03-15T21:03:49.971266Z"
        },
        "id": "LiDz6ABmxZkJ",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/users/u28/spencer/anaconda3/envs/llm2/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 7.730419158935547\n",
            "Epoch 10, Loss: 4.077061176300049\n",
            "Epoch 20, Loss: 1.6548950672149658\n",
            "Epoch 30, Loss: 0.33493417501449585\n",
            "Stopping early — loss < 0.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "real_prompt = \"A quick brown fox\"\n",
        "target_output = \"did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "full_text = real_prompt + \" \" + target_output\n",
        "\n",
        "# Disable special tokens\n",
        "input_ids = tokenizer(full_text, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
        "\n",
        "#fetch the number of virtual tokens\n",
        "num_virtual_tokens = model.peft_config[\"default\"].num_virtual_tokens\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    outputs = model(input_ids=input_ids)\n",
        "\n",
        "    # Align logits and targets\n",
        "    logits = outputs.logits[:, num_virtual_tokens:-1, :]  # skip virtual token outputs\n",
        "    targets = input_ids[:, 1:]   \n",
        "\n",
        "    # Check alignment\n",
        "    assert logits.shape[1] == targets.shape[1], (\n",
        "        f\"Shape mismatch before flatten: logits {logits.shape}, targets {targets.shape}\"\n",
        "    )\n",
        "\n",
        "    logits = logits.contiguous().view(-1, model.config.vocab_size)\n",
        "    targets = targets.contiguous().view(-1)\n",
        "\n",
        "    loss = F.cross_entropy(logits, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "    \n",
        "    if loss.item() < 0.1:\n",
        "        print(\"Stopping early — loss < 0.1\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-15T21:11:54.15018Z",
          "iopub.status.busy": "2025-03-15T21:11:54.149796Z",
          "iopub.status.idle": "2025-03-15T21:11:56.140523Z",
          "shell.execute_reply": "2025-03-15T21:11:56.139518Z",
          "shell.execute_reply.started": "2025-03-15T21:11:54.150141Z"
        },
        "id": "T4BPyw0_xZkJ",
        "outputId": "18ff73a0-47ab-43b0-af1b-8c4d9eab82fb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Generate output to verify model learning\n",
        "#model.eval()\n",
        "#with torch.no_grad():\n",
        "    #generated = model.generate(input_ids=input_ids[:, :num_virtual_tokens], max_new_tokens=6)\n",
        "    #print(tokenizer.decode(generated[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UW54GnzCwVpp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.07182963937520981\n"
          ]
        }
      ],
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False, add_special_tokens=False).to(device)\n",
        "outputs = model(**batch)\n",
        "\n",
        "logits = outputs.logits[:, num_virtual_tokens:-1, :]   \n",
        "targets = batch['input_ids'][:, 1:]                            \n",
        "loss = F.cross_entropy(logits.flatten(0, 1), targets.flatten(0, 1))\n",
        "\n",
        "print(\"Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCkpKYjWxfhk"
      },
      "source": [
        "### Parameter-efficient finetuning with LoRA (5 points)\n",
        "\n",
        "When training on more serious tasks, you can use low-rank adapters based on the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n",
        "\n",
        "The core idea is to add low-rank adapters __in parallel with existing linear layers,__ like this:\n",
        "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n",
        "\n",
        "In the original LoRA paper, the adapters were only added to attention projection matrices. However, [subsequent works](https://arxiv.org/abs/2305.14314) show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b87c54c3bed847ab933eae8175359169",
            "8aaa22971b3e416eae8394ef3b3b3f0f",
            "e9c4ba0d262c4b76baa00532e209b92f",
            "e6f9064e6ec545debe2e90163f4c712c",
            "6aad5b046def4a7db1048434e874b5d5",
            "dba48e929a2e43ec8e4bb3d4b32475ca",
            "530d66e4732b4d5486165654415bd2dc",
            "2bd4b6acd8004c1e98c064708108938e",
            "09b07105e4f54d2bb5e6cc1c1f1a9c8e",
            "923869a5864c4d3d80fb76c99fff24e2",
            "25e1f3d72230485c9b84cae4f685a69a",
            "a1fc119a899a4e5bbc6650b090a89c39"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T02:13:21.770903Z",
          "iopub.status.busy": "2025-03-16T02:13:21.770498Z",
          "iopub.status.idle": "2025-03-16T02:14:02.279526Z",
          "shell.execute_reply": "2025-03-16T02:14:02.278846Z",
          "shell.execute_reply.started": "2025-03-16T02:13:21.770871Z"
        },
        "id": "8zundaSzx90r",
        "outputId": "3faf7150-7685-4089-cf58-e03e4fce8bc6",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ee11b46f66140fa9f120a270c2e8ee5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# re-load the model to remove any previous PEFT tuners\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MJ_hq4fwyPVR"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module  # pre-trained (frozen) linear layer\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Frozen pre-trained linear layer output\n",
        "        original_output = self.module(input)\n",
        "\n",
        "        # LoRA adapter output: low-rank projection\n",
        "        adapter_output = input @ self.adapter_A @ self.adapter_B\n",
        "\n",
        "        # Return the sum of original and adapter\n",
        "        return original_output + adapter_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-15T21:44:06.320027Z",
          "iopub.status.busy": "2025-03-15T21:44:06.319783Z",
          "iopub.status.idle": "2025-03-15T21:44:06.330984Z",
          "shell.execute_reply": "2025-03-15T21:44:06.330146Z",
          "shell.execute_reply.started": "2025-03-15T21:44:06.320006Z"
        },
        "id": "tTzOs65JydcS",
        "outputId": "e07177c9-2f2b-432a-8a97-9e507df166bf",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# test your implementation\n",
        "test_linear = nn.Linear(128, 128)\n",
        "test_linear.weight.data[...] = torch.eye(128)\n",
        "test_adapter = LoRALayer(test_linear, rank=8)\n",
        "\n",
        "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
        "\n",
        "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
        "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
        "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
        "\n",
        "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
        "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
        "dummy_loss.backward()\n",
        "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
        "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
        "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
        "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
        "del dummy_loss, test_linear, test_adapter\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tajVTsvLulB6"
      },
      "source": [
        "### Apply LoRA to the model\n",
        "\n",
        "The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n",
        "* self_attn.o_proj - attention output projection\n",
        "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n",
        "* lm_head - output LM head\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T02:14:02.287591Z",
          "iopub.status.busy": "2025-03-16T02:14:02.287304Z",
          "iopub.status.idle": "2025-03-16T02:14:02.322868Z",
          "shell.execute_reply": "2025-03-16T02:14:02.322276Z",
          "shell.execute_reply.started": "2025-03-16T02:14:02.287565Z"
        },
        "id": "davyUVEwulB6",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'LoRALayer' object has no attribute 'in_features'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m model.model.layers.named_modules():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mLlamaDecoderLayer\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(module)):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         module.self_attn.q_proj = \u001b[43mLoRALayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_rank\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      6\u001b[39m         module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n\u001b[32m      7\u001b[39m         module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mLoRALayer.__init__\u001b[39m\u001b[34m(self, module, rank)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.module = module  \u001b[38;5;66;03m# pre-trained (frozen) linear layer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mself\u001b[39m.adapter_A = nn.Parameter(torch.empty(\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43min_features\u001b[49m, rank, device=module.weight.device))\n\u001b[32m      7\u001b[39m nn.init.kaiming_uniform_(\u001b[38;5;28mself\u001b[39m.adapter_A, a=\u001b[32m5\u001b[39m ** \u001b[32m0.5\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm2/lib/python3.11/site-packages/torch/nn/modules/module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
            "\u001b[31mAttributeError\u001b[39m: 'LoRALayer' object has no attribute 'in_features'"
          ]
        }
      ],
      "source": [
        "lora_rank = 8\n",
        "\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "\n",
        "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-15T21:44:42.162967Z",
          "iopub.status.busy": "2025-03-15T21:44:42.162673Z",
          "iopub.status.idle": "2025-03-15T21:44:42.8499Z",
          "shell.execute_reply": "2025-03-15T21:44:42.848889Z",
          "shell.execute_reply.started": "2025-03-15T21:44:42.162945Z"
        },
        "id": "AWzfvc0EulB6",
        "outputId": "b432afe7-08b9-4cb2-c6ac-01f85352a689",
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "# test a single training step, make sure we get meaningful gradients\n",
        "with torch.amp.autocast(device_type = 'cuda', dtype=torch.float32):\n",
        "    out = model.forward(**batch)\n",
        "    (out.logits.norm() / 100).backward()\n",
        "\n",
        "for i, module in enumerate(model.modules()):\n",
        "    if isinstance(module, LoRALayer):\n",
        "        assert module.adapter_B.grad is not None\n",
        "        assert module.adapter_B.grad.norm().item() > 0\n",
        "\n",
        "model.zero_grad(set_to_none=True)\n",
        "print(\"Grad check successful, well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjIJ1vkUulB7"
      },
      "source": [
        "### (example) How to train your model\n",
        "\n",
        "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "93ec6c07bea146ecbf5cfa9c91214afe",
            "eb473a48f4c443e4b8fce455d18ceaed",
            "8d74a2a77ef3474d94fbfdf682942475",
            "471b953c29794dad84adadaca18a8306",
            "8fc6f05cd3ce4434b7de1fd9aedeaca8",
            "93309656277b4f7c92dcdae32c9deb0f",
            "55ea330dd33f4b3f9c7249f6b2a72295",
            "08d066dca79e4922a9f2baa85705d83d",
            "2574861521a74595ab8c729209425177",
            "5244d74cf105401fb9d83eafcfefcfd7",
            "5d0f4d57848c4c71aab57d260501e43e"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T02:11:47.55534Z",
          "iopub.status.busy": "2025-03-16T02:11:47.554998Z",
          "iopub.status.idle": "2025-03-16T02:11:47.594339Z",
          "shell.execute_reply": "2025-03-16T02:11:47.593078Z",
          "shell.execute_reply.started": "2025-03-16T02:11:47.555319Z"
        },
        "id": "NvmLOj2oxZkK",
        "outputId": "b9097272-4a9e-4674-abb7-8c69c33b9b7f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "# Define prompt and completion\n",
        "texts = [\"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"]\n",
        "\n",
        "# Tokenizing the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=30)\n",
        "\n",
        "# Create a dataset\n",
        "dataset = Dataset.from_dict({\"text\": texts})\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Drop the original text column (optional)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T02:21:28.986757Z",
          "iopub.status.busy": "2025-03-16T02:21:28.986382Z",
          "iopub.status.idle": "2025-03-16T02:23:44.609219Z",
          "shell.execute_reply": "2025-03-16T02:23:44.607883Z",
          "shell.execute_reply.started": "2025-03-16T02:21:28.986727Z"
        },
        "id": "r9mIpntHulB8",
        "outputId": "1d838126-eddc-4543-dc78-ba8144b37b76",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model._hf_peft_config_loaded = True  # silence a warning from HF trainer for hand-made LoRA\n",
        "\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, train_dataset=tokenized_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1, gradient_accumulation_steps=4,\n",
        "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
        "        warmup_steps=250, max_steps=50, learning_rate=2e-4, fp16=True,\n",
        "        logging_steps=1, output_dir='outputs', report_to=\"none\"), # Unse repotr_to=None for Wandb\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HofX-vgQsBOE"
      },
      "source": [
        "You'll gett an error after training, but you still can do inference with this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T02:23:59.162256Z",
          "iopub.status.busy": "2025-03-16T02:23:59.16195Z",
          "iopub.status.idle": "2025-03-16T02:23:59.172448Z",
          "shell.execute_reply": "2025-03-16T02:23:59.171589Z",
          "shell.execute_reply.started": "2025-03-16T02:23:59.162233Z"
        },
        "id": "2A0lAOtCxZkK",
        "outputId": "69a0e512-f86e-4d25-a1a6-6214b1e9fb6b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = trainer.model\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T02:24:06.288473Z",
          "iopub.status.busy": "2025-03-16T02:24:06.288143Z",
          "iopub.status.idle": "2025-03-16T02:24:08.262599Z",
          "shell.execute_reply": "2025-03-16T02:24:08.261756Z",
          "shell.execute_reply.started": "2025-03-16T02:24:06.288443Z"
        },
        "id": "IeXF3VwZxZkK",
        "outputId": "3a0defb8-4d00-4413-9abc-4d1eaf1bf4be",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\\nThe quick brown fox']\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(['A quick brown fox'],\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=23, eos_token_id=3\n",
        "    )\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final task: *actually* train the model (4 points)\n",
        "\n",
        "Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n",
        "\n",
        "* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter validation subset of `codeparrots`\n",
        "* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n",
        "* __short lines:__ please take the first 512 characters of each line\n",
        "* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n",
        "   - extra adapter on lm_head\n",
        "   - extra adapter on MLP components (mlp.*)\n",
        "   - trainable input embeddings (requires tweaking memory usage)\n",
        "\n",
        "* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n",
        "\n",
        "\n",
        "Note: the peft library also has LoRA implementation. However, we ask that for this assignment you show at least one complete training run with your own LoRA code.\n",
        "\n",
        "__Alternative assignment:__ Instead of doing python code, feel free to substitute the task with any other dataset, e.g. your favorite artist or podcast, as long as it's ethical. If you choose your own task, please show examples of what your model learned - or did not learn, akin to the code examples below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install --quiet --upgrade transformers bitsandbytes accelerate sentencepiece optimum auto-gptq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import BitsAndBytesConfig\n",
        "from tqdm.auto import tqdm, trange\n",
        "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # use 'cuda' for any GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c33f19c01c3547d79fc05fb4bf7fcc2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b624e198a60b46e68e1e75ef91f02449",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/108 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the codeparrot dataset\n",
        "ds = load_dataset(\"codeparrot/codeparrot-clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Truncated each line of the content section to 512 characters and removed empty lines, comments, and docstrings\n",
        "def split_lines(example):\n",
        "    lines = example[\"content\"].splitlines()\n",
        "    cleanLines = []\n",
        "    in_block = False\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        tripple_quote_count = line.count(\"'''\") + line.count('\"\"\"')\n",
        "        \n",
        "        if tripple_quote_count % 2 == 1:\n",
        "            in_block = not in_block\n",
        "            continue\n",
        "        \n",
        "        if in_block or tripple_quote_count >= 2:\n",
        "            continue\n",
        "        \n",
        "        if line.startswith(\"#\"):\n",
        "            continue\n",
        "        \n",
        "        cleanLines.append(line[:512])\n",
        "        \n",
        "    return {\"content\": cleanLines}\n",
        "\n",
        "# split_dataset_2 = ds.map(split_lines)\n",
        "split_dataset_2 = ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'repo_name': 'ahmedbodi/AutobahnPython', 'path': 'examples/asyncio/websocket/echo/client_coroutines.py', 'copies': '13', 'size': '2044', 'content': '###############################################################################\\n##\\n##  Copyright (C) 2013-2014 Tavendo GmbH\\n##\\n##  Licensed under the Apache License, Version 2.0 (the \"License\");\\n##  you may not use this file except in compliance with the License.\\n##  You may obtain a copy of the License at\\n##\\n##      http://www.apache.org/licenses/LICENSE-2.0\\n##\\n##  Unless required by applicable law or agreed to in writing, software\\n##  distributed under the License is distributed on an \"AS IS\" BASIS,\\n##  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n##  See the License for the specific language governing permissions and\\n##  limitations under the License.\\n##\\n###############################################################################\\n\\nfrom autobahn.asyncio.websocket import WebSocketClientProtocol, \\\\\\n                                       WebSocketClientFactory\\n\\nimport asyncio\\n\\n\\n\\nclass MyClientProtocol(WebSocketClientProtocol):\\n\\n   def onConnect(self, response):\\n      print(\"Server connected: {0}\".format(response.peer))\\n\\n   @asyncio.coroutine\\n   def onOpen(self):\\n      print(\"WebSocket connection open.\")\\n\\n      ## start sending messages every second ..\\n      while True:\\n         self.sendMessage(u\"Hello, world!\".encode(\\'utf8\\'))\\n         self.sendMessage(b\"\\\\x00\\\\x01\\\\x03\\\\x04\", isBinary = True)\\n         yield from asyncio.sleep(1)\\n\\n   def onMessage(self, payload, isBinary):\\n      if isBinary:\\n         print(\"Binary message received: {0} bytes\".format(len(payload)))\\n      else:\\n         print(\"Text message received: {0}\".format(payload.decode(\\'utf8\\')))\\n\\n   def onClose(self, wasClean, code, reason):\\n      print(\"WebSocket connection closed: {0}\".format(reason))\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n\\n   import asyncio\\n\\n   factory = WebSocketClientFactory(\"ws://localhost:9000\", debug = False)\\n   factory.protocol = MyClientProtocol\\n\\n   loop = asyncio.get_event_loop()\\n   coro = loop.create_connection(factory, \\'127.0.0.1\\', 9000)\\n   loop.run_until_complete(coro)\\n   loop.run_forever()\\n   loop.close()\\n', 'license': 'apache-2.0', 'hash': 7822061744094950801, 'line_mean': 31.4444444444, 'line_max': 79, 'alpha_frac': 0.6232876712, 'autogenerated': False}\n"
          ]
        }
      ],
      "source": [
        "print(split_dataset_2['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5361373\n"
          ]
        }
      ],
      "source": [
        "print(len(split_dataset_2['train']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36e709b9197d4b5b8aaaf736aeb7551c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load model\n",
        "model_name = 'codellama/CodeLlama-7b-hf'\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizing the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "# Create a dataset\n",
        "tokenized_dataset_2 = split_dataset_2.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module  # pre-trained (frozen) linear layer\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Frozen pre-trained linear layer output\n",
        "        original_output = self.module(input)\n",
        "\n",
        "        # LoRA adapter output: low-rank projection\n",
        "        adapter_output = input @ self.adapter_A @ self.adapter_B\n",
        "\n",
        "        # Return the sum of original and adapter\n",
        "        return original_output + adapter_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test LoRA implementation\n",
        "lora_rank = 8\n",
        "\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "\n",
        "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B\n",
        "\n",
        "# Add LoRALayer to the lm_head as well\n",
        "model.lm_head = LoRALayer(model.lm_head, rank=lora_rank).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = tokenizer(tokenized_dataset_2['train'][0]['content'], return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "# Test a single training step, make sure we get meaningful gradients\n",
        "with torch.amp.autocast(device_type = 'cuda', dtype=torch.float32):\n",
        "    out = model.forward(**batch)\n",
        "    (out.logits.norm() / 100).backward()\n",
        "\n",
        "for i, module in enumerate(model.modules()):\n",
        "    if isinstance(module, LoRALayer):\n",
        "        assert module.adapter_B.grad is not None\n",
        "        assert module.adapter_B.grad.norm().item() > 0\n",
        "\n",
        "model.zero_grad(set_to_none=True)\n",
        "print(\"Grad check successful, well done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenized_dataset_2' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39m_hf_peft_config_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Set up optimizer to only update PEFT parameters\u001b[39;00m\n\u001b[0;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m----> 6\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_dataset_2\u001b[49m,\n\u001b[0;32m      7\u001b[0m     args\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[0;32m      8\u001b[0m         per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gradient_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# note: if you want larger batch size, increase gradient_accumulation_steps\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-4\u001b[39m, fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m         logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;66;03m# Unse repotr_to=None for Wandb\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# If you see cache warnings, set `model.config.use_cache = False` to silence them (re-enable for inference)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tokenized_dataset_2' is not defined"
          ]
        }
      ],
      "source": [
        "# Silence a warning from HF trainer for hand-made LoRA\n",
        "model._hf_peft_config_loaded = True\n",
        "\n",
        "# Set up optimizer to only update PEFT parameters\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, train_dataset=tokenized_dataset_2,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1, gradient_accumulation_steps=4,\n",
        "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
        "        warmup_steps=250, max_steps=50, learning_rate=2e-4, fp16=True,\n",
        "        logging_steps=1, output_dir='outputs', report_to=\"none\"), # Unse repotr_to=None for Wandb\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "# If you see cache warnings, set `model.config.use_cache = False` to silence them (re-enable for inference)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = trainer.model\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
        "\n",
        "inputs = tokenizer(prompts,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=23, eos_token_id=3\n",
        "    )\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
        "\n",
        "# <A WHOLE LOT OF YOUR CODE>\n",
        "# generate baseline samples with the selected prompts before finetuning\n",
        "# please feel free to use transformers.Trainer (as above) or your custom training code\n",
        "# after the training concludes, please show examples of text generated by your model. It is expected to look like Python code fragments\n",
        "# print the generation examples nicely (suggestion: use pandas or HTML) for easier comparison\n",
        "# note: your LoRA-enhanced model can run generation the same way as the non-trained model (above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This template helps to compare generated code samples in pretty table form\n",
        "# feel free to present your work in other forms\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
        "  <tr>\n",
        "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
        "  </tr>\n",
        "{}\n",
        "</table>\"\"\"\n",
        "\n",
        "row_template = '''  <tr>\n",
        "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "  </tr>'''\n",
        "\n",
        "rows = []\n",
        "\n",
        "for prompt in prompts:\n",
        "    # replace placeholders in the format() arguments\n",
        "    rows.append(row_template.format(prompt, \"BEFORE FINETUNING\", \"TO BE GENERATED AFTER FINETUNING\"))\n",
        "\n",
        "display(HTML(table_template.format('\\n'.join(rows))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you reach this: congratulations! you've completed everything in this practice session.\n",
        "\n",
        "If you want to dig deeper, try to implement prompt-tuning (for bonus points!).\n",
        "You can read more about prompt tuning variants in paper [1](https://arxiv.org/abs/2104.08691) or paper [2](https://arxiv.org/abs/2101.00190). Both versions can be implemented by passing trainable prompts as `model.forward(..., past_key_values=your_prompts)`.\n",
        "\n",
        "\n",
        "\n",
        "### Read more\n",
        "\n",
        "* How post-training quantization works: https://arxiv.org/abs/2208.07339\n",
        "* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n",
        "* A general library for different adapter types: https://adapterhub.ml/\n",
        "\n",
        "\n",
        "### [extra info] Running other models.\n",
        "\n",
        "This notebook's code can run with other models of similar size, such as [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [OPT-6.7B](https://huggingface.co/facebook/opt-6.7b) or [BLOOM-7.1B](https://huggingface.co/bigscience/bloom-7b1). However, they will require minor code tweaks:\n",
        "1. change the model name in `AutoModelForCausalLM.from_pretrained()` __and__ `AutoTokenizer`\n",
        "2. In the prompt tuning code, change `model.model.embed_tokens` to refer to the target model's word embeddings. Simply `print(model)` to navigate to them.\n",
        "3. Change code to add Lora layers - specifically where you what the transformer block components, since those components now have different names."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "notebook58e9291559",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08d066dca79e4922a9f2baa85705d83d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b07105e4f54d2bb5e6cc1c1f1a9c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2574861521a74595ab8c729209425177": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25e1f3d72230485c9b84cae4f685a69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd4b6acd8004c1e98c064708108938e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "471b953c29794dad84adadaca18a8306": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5244d74cf105401fb9d83eafcfefcfd7",
            "placeholder": "​",
            "style": "IPY_MODEL_5d0f4d57848c4c71aab57d260501e43e",
            "value": " 1/1 [00:00&lt;00:00, 12.81 examples/s]"
          }
        },
        "5244d74cf105401fb9d83eafcfefcfd7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "530d66e4732b4d5486165654415bd2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55ea330dd33f4b3f9c7249f6b2a72295": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d0f4d57848c4c71aab57d260501e43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aad5b046def4a7db1048434e874b5d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aaa22971b3e416eae8394ef3b3b3f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba48e929a2e43ec8e4bb3d4b32475ca",
            "placeholder": "​",
            "style": "IPY_MODEL_530d66e4732b4d5486165654415bd2dc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8d74a2a77ef3474d94fbfdf682942475": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08d066dca79e4922a9f2baa85705d83d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2574861521a74595ab8c729209425177",
            "value": 1
          }
        },
        "8fc6f05cd3ce4434b7de1fd9aedeaca8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "923869a5864c4d3d80fb76c99fff24e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93309656277b4f7c92dcdae32c9deb0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93ec6c07bea146ecbf5cfa9c91214afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb473a48f4c443e4b8fce455d18ceaed",
              "IPY_MODEL_8d74a2a77ef3474d94fbfdf682942475",
              "IPY_MODEL_471b953c29794dad84adadaca18a8306"
            ],
            "layout": "IPY_MODEL_8fc6f05cd3ce4434b7de1fd9aedeaca8"
          }
        },
        "b87c54c3bed847ab933eae8175359169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8aaa22971b3e416eae8394ef3b3b3f0f",
              "IPY_MODEL_e9c4ba0d262c4b76baa00532e209b92f",
              "IPY_MODEL_e6f9064e6ec545debe2e90163f4c712c"
            ],
            "layout": "IPY_MODEL_6aad5b046def4a7db1048434e874b5d5"
          }
        },
        "dba48e929a2e43ec8e4bb3d4b32475ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6f9064e6ec545debe2e90163f4c712c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_923869a5864c4d3d80fb76c99fff24e2",
            "placeholder": "​",
            "style": "IPY_MODEL_25e1f3d72230485c9b84cae4f685a69a",
            "value": " 33/33 [01:49&lt;00:00,  3.12s/it]"
          }
        },
        "e9c4ba0d262c4b76baa00532e209b92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd4b6acd8004c1e98c064708108938e",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09b07105e4f54d2bb5e6cc1c1f1a9c8e",
            "value": 33
          }
        },
        "eb473a48f4c443e4b8fce455d18ceaed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93309656277b4f7c92dcdae32c9deb0f",
            "placeholder": "​",
            "style": "IPY_MODEL_55ea330dd33f4b3f9c7249f6b2a72295",
            "value": "Map: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
